{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "#### Baseline model features:\n",
    "    - each row represents a day of data, where each category of feature has 24 columns corresponding to 24 hours in a day\n",
    "    \n",
    "#### RNN/LSTM model features:\n",
    "    - Engineered 3d data corresponding to (batch_size, time_steps, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepending /home/dyllanjr/anaconda3/envs/solarenv/lib/python3.6/site-packages/bigdl/share/conf/spark-bigdl.conf to sys.path\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "from pyspark import SparkContext\n",
    "from bigdl.util.common import *\n",
    "\n",
    "\n",
    "# create sparkcontext with bigdl configuration\n",
    "sc = SparkContext.getOrCreate(conf=create_spark_conf().setMaster(\"local[*]\"))\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sparkContext = sc)\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(f\"hdfs://localhost:9000/solar_data/interim/Arkansas_interim.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Minute: integer (nullable = true)\n",
      " |-- Cloud Type: integer (nullable = true)\n",
      " |-- Dew Point: integer (nullable = true)\n",
      " |-- Fill Flag: integer (nullable = true)\n",
      " |-- Wind Speed: double (nullable = true)\n",
      " |-- Surface Albedo: double (nullable = true)\n",
      " |-- Temperature: integer (nullable = true)\n",
      " |-- Solar Zenith Angle: double (nullable = true)\n",
      " |-- Wind Direction: double (nullable = true)\n",
      " |-- GHI: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|minute|\n",
      "+------+\n",
      "|    30|\n",
      "|     0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('minute').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The predicted column will be the sum of GHI for every half hour in the day\n",
    "#the average daily Wh/M^2 is around 4000, so these summed GHI values make sense.\n",
    "ghi_df = (df.groupby(F.year(F.col('timestamp')).alias('year'),\n",
    "            F.month(F.col('timestamp')).alias('month'), \n",
    "            F.dayofmonth(F.col('timestamp')).alias('day'))\n",
    "     .sum('GHI')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###We want each row to contain information about every hour of the day\n",
    "#Note that the data is every half hour, but I'll be simplifying to every hour for implementation's sake.\n",
    "#What I have to do is pivot where my index is the unique year/month/day combination\n",
    "#the columns to pivot will be hour and minute\n",
    "#the values will be \n",
    "cols = ['Cloud Type', 'Dew Point', 'Wind Speed', 'Surface Albedo', 'Temperature', 'Solar Zenith Angle', 'Wind Direction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfpd = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#Applying the box-cox transformation to normalize features\n",
    "for col in cols:\n",
    "    maxVal = dfpd[col][dfpd[col].idxmax()]\n",
    "    minVal = dfpd[col][dfpd[col].idxmin()]\n",
    "    \n",
    "    col_bct, l = stats.boxcox(dfpd[col]- minVal +1)\n",
    "    col_bct = col_bct*l/((maxVal +1)**l-1)\n",
    "    col_bct =pd.Series(col_bct)\n",
    "    dfpd[col] = col_bct\n",
    "    \n",
    "df = spark.createDataFrame(dfpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#copied from above for ease of copying in future\n",
    "def rename_cols(feature, df):\n",
    "    for hour in range(0, 24):\n",
    "        df = df.withColumnRenamed(str(hour), feature.replace(' ', '_')+'_'+str(hour))\n",
    "    return df\n",
    "\n",
    "def generate_feature_pivot(feature, df):\n",
    "    pivoted_df = (df.groupby(F.year(F.col('timestamp')).alias('year'),\n",
    "                        F.month(F.col('timestamp')).alias('month'), \n",
    "                        F.dayofmonth(F.col('timestamp')).alias('day'))\n",
    "                 .pivot('Hour')\n",
    "                 .agg(F.first(feature)) #There will  be two values in this group, the first is right on the hour\n",
    "            )\n",
    "    pivoted_df = rename_cols(feature=feature, df=pivoted_df)\n",
    "    return pivoted_df\n",
    "\n",
    "#Finally I'll make one big convenience function to do the same with future datasets.\n",
    "def process_data(df):\n",
    "    ghi_df = (df.groupby(F.year(F.col('timestamp')).alias('year'),\n",
    "                F.month(F.col('timestamp')).alias('month'), \n",
    "                F.dayofmonth(F.col('timestamp')).alias('day'))\n",
    "         .sum('GHI')\n",
    "    )\n",
    "    \n",
    "    cols = ['Cloud Type', 'Dew Point', 'Wind Speed', 'Surface Albedo', 'Temperature', 'Solar Zenith Angle', 'Wind Direction']\n",
    "    #take a count of each pivot and final join to see if inner join lost any rows.\n",
    "    counts = []\n",
    "    for ix, col in enumerate(cols):\n",
    "        if ix == 0:\n",
    "            joined = generate_feature_pivot(feature=col, df=df)\n",
    "            counts.append(joined.count())\n",
    "            #to keep track of progress\n",
    "            print(ix)\n",
    "        else:\n",
    "            joined2 = generate_feature_pivot(feature=col, df=df)\n",
    "            counts.append(joined2.count())\n",
    "            joined = joined.join(joined2, on=['year', 'month', 'day'])\n",
    "            print(ix)\n",
    "    joined = joined.join(ghi_df, on=['year', 'month', 'day'])\n",
    "    return joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "names = ['Arkansas', 'Arizona', 'Georgia']\n",
    "for name in names:\n",
    "    df = spark.read.csv(f\"hdfs://localhost:9000/solar_data/interim/{name}_interim.csv\", inferSchema=True, header=True)\n",
    "    joined = process_data(df)\n",
    "    joined.write.csv(f\"/home/dyllanjr/Solar_Irradiance_Prediction/data/processed/{name}_1.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### RNN/LSTM data preperation\n",
    "#https://stackoverflow.com/questions/57549011/lstm-keras-input-shape-confusion\n",
    "def create_windows(data, window_shape, step = 1, start_id = None, end_id = None):\n",
    "    \n",
    "    data = np.asarray(data)\n",
    "    data = data.reshape(-1,1) if np.prod(data.shape) == max(data.shape) else data\n",
    "        \n",
    "    start_id = 0 if start_id is None else start_id\n",
    "    end_id = data.shape[0] if end_id is None else end_id\n",
    "    \n",
    "    data = data[int(start_id):int(end_id),:]\n",
    "    window_shape = (int(window_shape), data.shape[-1])\n",
    "    step = (int(step),) * data.ndim\n",
    "    slices = tuple(slice(None, None, st) for st in step)\n",
    "    indexing_strides = data[slices].strides\n",
    "    win_indices_shape = ((np.array(data.shape) - window_shape) // step) + 1\n",
    "    \n",
    "    new_shape = tuple(list(win_indices_shape) + list(window_shape))\n",
    "    strides = tuple(list(indexing_strides) + list(data.strides))\n",
    "    \n",
    "    window_data = np.lib.stride_tricks.as_strided(data, shape=new_shape, strides=strides)\n",
    "    \n",
    "    return np.squeeze(window_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(f\"hdfs://localhost:9000/solar_data/interim/Arkansas_interim.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make sure that the dataframe is sorted by timestamp\n",
    "df = df.sort(F.col('timestamp').asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "cols.remove('Year')\n",
    "cols.remove('Month')\n",
    "cols.remove('Day')\n",
    "cols.remove('Hour')\n",
    "cols.remove('Minute')\n",
    "cols.remove('timestamp')\n",
    "cols.remove('GHI')\n",
    "\n",
    "Xs = df.select(cols).toPandas().as_matrix()\n",
    "ys = df.select('GHI').toPandas().as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### We want a 48-step ahead forecast (corresponding to one day) with a 48,48*2,48*3-step look-back (corresponding to one/two/three days)\n",
    "look_back = 48\n",
    "look_ahead = 48\n",
    "\n",
    "X_seq = create_windows(Xs, window_shape = look_back, end_id= -look_ahead, step = 48)\n",
    "y_seq = create_windows(ys, window_shape = look_ahead, start_id= 0, step = 48)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Day', 'Month', 'Year']).count().count()\n",
    "#This makes sense since we lose one day by looking ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/dyllanjr/Solar_Irradiance_Prediction/data/processed/\n",
    "\n",
    "#I'll save each of them as a .npy file, and paralellize upon loading\n",
    "names = ['Arkansas', 'Arizona', 'Georgia']\n",
    "look_backs = [48, 48*2, 48*3]\n",
    "look_ahead = 48\n",
    "\n",
    "for name in names:\n",
    "    for look_back in look_backs:\n",
    "        df = spark.read.csv(f\"hdfs://localhost:9000/solar_data/interim/{name}_interim.csv\", inferSchema=True, header=True)\n",
    "        #Make sure that the dataframe is sorted by timestamp\n",
    "        df = df.sort(F.col('timestamp').asc())\n",
    "        cols = df.columns\n",
    "        cols.remove('Year')\n",
    "        cols.remove('Month')\n",
    "        cols.remove('Day')\n",
    "        cols.remove('Hour')\n",
    "        cols.remove('Minute')\n",
    "        cols.remove('timestamp')\n",
    "        cols.remove('GHI')\n",
    "\n",
    "        Xs = df.select(cols).toPandas().as_matrix()\n",
    "        ys = df.select('GHI').toPandas().as_matrix()\n",
    "        \n",
    "        X_seq = create_windows(Xs, window_shape = look_back, end_id= -look_ahead, step = 48)\n",
    "        y_seq = create_windows(ys, window_shape = look_ahead, start_id=0, step = 48)\n",
    "        \n",
    "        np.savez(f'{name}_look_back_{look_back}', x=X_seq, y=y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('hadoop fs -copyFromLocal /home/dyllanjr/Solar_Irradiance_Prediction/data/ /solar_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
