{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "### Note:\n",
    "    - This is not the final stage of the data.\n",
    "    - This is an interim stage before data is processed for supervised time series model training\n",
    "    - Will do EDA on this interim stage becuase of readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from bigdl.util.common import *\n",
    "\n",
    "\n",
    "# create sparkcontext with bigdl configuration\n",
    "sc = SparkContext.getOrCreate(conf=create_spark_conf().setMaster(\"local[*]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sparkContext = sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gather Arkansas Dataset for cleansing\n",
    "years = list(range(1998, 2018))\n",
    "for ix, year in enumerate(years):\n",
    "    if ix == 0:\n",
    "        df = spark.read.csv(f\"hdfs://localhost:9000/solar_data/raw/Arkansas_{year}.csv\", inferSchema=True, header=True)\n",
    "    else:\n",
    "        to_append = spark.read.csv(f\"hdfs://localhost:9000/solar_data/raw/Arkansas_{year}.csv\", inferSchema=True, header=True)\n",
    "        df = df.union(to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----+---+----+------+----------+---------+---------+------------------+--------------+-----------+------------------+--------------+---+\n",
      "|                _c0|Year|Month|Day|Hour|Minute|Cloud Type|Dew Point|Fill Flag|        Wind Speed|Surface Albedo|Temperature|Solar Zenith Angle|Wind Direction|GHI|\n",
      "+-------------------+----+-----+---+----+------+----------+---------+---------+------------------+--------------+-----------+------------------+--------------+---+\n",
      "|1998-01-01 05:00:00|1998|    1|  1|   0|     0|         0|       -5|        0|0.6000000000000001|         0.114|          3|             101.6|         182.9|  0|\n",
      "|1998-01-01 05:30:00|1998|    1|  1|   0|    30|         0|       -5|        0|               0.8|         0.114|          2|            107.41|         182.9|  0|\n",
      "|1998-01-01 06:00:00|1998|    1|  1|   1|     0|         0|       -5|        0|               1.1|         0.114|          2|            113.34|         172.5|  0|\n",
      "|1998-01-01 06:30:00|1998|    1|  1|   1|    30|         0|       -5|        0|               1.1|         0.114|          1|            119.36|         172.5|  0|\n",
      "|1998-01-01 07:00:00|1998|    1|  1|   2|     0|         0|       -4|        0|1.2000000000000002|         0.114|          0|            125.43|         172.0|  0|\n",
      "+-------------------+----+-----+---+----+------+----------+---------+---------+------------------+--------------+-----------+------------------+--------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350400"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('_c0', 'timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Minute: integer (nullable = true)\n",
      " |-- Cloud Type: integer (nullable = true)\n",
      " |-- Dew Point: integer (nullable = true)\n",
      " |-- Fill Flag: integer (nullable = true)\n",
      " |-- Wind Speed: double (nullable = true)\n",
      " |-- Surface Albedo: double (nullable = true)\n",
      " |-- Temperature: integer (nullable = true)\n",
      " |-- Solar Zenith Angle: double (nullable = true)\n",
      " |-- Wind Direction: double (nullable = true)\n",
      " |-- GHI: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|Fill Flag| count|\n",
      "+---------+------+\n",
      "|        1|  9936|\n",
      "|        3|   589|\n",
      "|        4| 15525|\n",
      "|        2|     3|\n",
      "|        0|324347|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Fill Flag').groupBy('Fill Flag').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Flag Meaning:\n",
    "\n",
    "\n",
    "0: NaN (no flags raise)   \n",
    "1: Missing Image \t\n",
    "2: Low Irradiance \t\n",
    "3: Exceeds Clearsky \t\n",
    "4: Missing CLoud Properties (does not mean cloud type is Nan as shown below)  \n",
    "5: Rayleigh Violation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|Cloud Type| count|\n",
      "+----------+------+\n",
      "|         1| 28995|\n",
      "|         6| 19685|\n",
      "|         3| 29608|\n",
      "|         9|  1557|\n",
      "|         4| 36421|\n",
      "|         8| 24064|\n",
      "|         7| 50712|\n",
      "|        10|     6|\n",
      "|         2|  5680|\n",
      "|         0|153672|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Cloud Type').groupBy('Cloud Type').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Type Meaning:\n",
    "\n",
    "-15: NaN  \n",
    "0: Clear   \n",
    "1: Probably Clear \n",
    "2: Fog \n",
    "3: Water  \n",
    "4: Super-Cooled Water \n",
    "5: Mixed \n",
    "6: Opaque Ice  \n",
    "7: Cirrus  \n",
    "8: Overlapping  \n",
    "9: Overshooting  \n",
    "10: Unknown \t \n",
    "11: Dust  \n",
    "12: Smoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|          timestamp|count|\n",
      "+-------------------+-----+\n",
      "|1998-04-05 07:00:00|    2|\n",
      "|1998-04-05 07:30:00|    2|\n",
      "|1999-04-04 07:00:00|    2|\n",
      "|1999-04-04 07:30:00|    2|\n",
      "|2000-04-02 07:00:00|    2|\n",
      "|2000-04-02 07:30:00|    2|\n",
      "|2001-04-01 07:00:00|    2|\n",
      "|2001-04-01 07:30:00|    2|\n",
      "|2002-04-07 07:00:00|    2|\n",
      "|2002-04-07 07:30:00|    2|\n",
      "|2003-04-06 07:00:00|    2|\n",
      "|2003-04-06 07:30:00|    2|\n",
      "|2004-04-04 07:00:00|    2|\n",
      "|2004-04-04 07:30:00|    2|\n",
      "|2005-04-03 07:00:00|    2|\n",
      "|2005-04-03 07:30:00|    2|\n",
      "|2006-04-02 07:00:00|    2|\n",
      "|2006-04-02 07:30:00|    2|\n",
      "|2007-03-11 07:00:00|    2|\n",
      "|2007-03-11 07:30:00|    2|\n",
      "|2008-03-09 07:00:00|    2|\n",
      "|2008-03-09 07:30:00|    2|\n",
      "|2009-03-08 07:00:00|    2|\n",
      "|2009-03-08 07:30:00|    2|\n",
      "|2010-03-14 07:00:00|    2|\n",
      "|2010-03-14 07:30:00|    2|\n",
      "|2011-03-13 07:00:00|    2|\n",
      "|2011-03-13 07:30:00|    2|\n",
      "|2012-03-11 07:00:00|    2|\n",
      "|2012-03-11 07:30:00|    2|\n",
      "|2013-03-10 07:00:00|    2|\n",
      "|2013-03-10 07:30:00|    2|\n",
      "|2014-03-09 07:00:00|    2|\n",
      "|2014-03-09 07:30:00|    2|\n",
      "|2015-03-08 07:00:00|    2|\n",
      "|2015-03-08 07:30:00|    2|\n",
      "|2016-03-13 07:00:00|    2|\n",
      "|2016-03-13 07:30:00|    2|\n",
      "|2017-03-12 07:00:00|    2|\n",
      "|2017-03-12 07:30:00|    2|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#40 unique duplicate entries!\n",
    "df.groupby('timestamp').count().where('count > 1').sort('count', ascending=False).sort('timestamp').show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(years)\n",
    "#the number of duplicates = 4 * len(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----+---+----+------+----------+---------+---------+----------+--------------+-----------+------------------+--------------+---+\n",
      "|          timestamp|Year|Month|Day|Hour|Minute|Cloud Type|Dew Point|Fill Flag|Wind Speed|Surface Albedo|Temperature|Solar Zenith Angle|Wind Direction|GHI|\n",
      "+-------------------+----+-----+---+----+------+----------+---------+---------+----------+--------------+-----------+------------------+--------------+---+\n",
      "|1998-04-05 03:00:00|1998|    4|  4|  22|     0|         4|        4|        0|       2.5|         0.183|         13|             60.84|          22.1|139|\n",
      "+-------------------+----+-----+---+----+------+----------+---------+---------+----------+--------------+-----------+------------------+--------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where('timestamp == \"1998-04-05 03:00:00\"').show()\n",
    "#The hour column doesn't match the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----+---+----+------+----------+---------+---------+----------+--------------+-----------+------------------+--------------+---+\n",
      "|          timestamp|Year|Month|Day|Hour|Minute|Cloud Type|Dew Point|Fill Flag|Wind Speed|Surface Albedo|Temperature|Solar Zenith Angle|Wind Direction|GHI|\n",
      "+-------------------+----+-----+---+----+------+----------+---------+---------+----------+--------------+-----------+------------------+--------------+---+\n",
      "|2014-03-09 03:00:00|2014|    3|  8|  22|     0|         7|       10|        0|       0.8|         0.166|         16|             66.09|         236.1|320|\n",
      "+-------------------+----+-----+---+----+------+----------+---------+---------+----------+--------------+-----------+------------------+--------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where('timestamp == \"2014-03-09 03:00:00\"').show()\n",
    "#The hour column doesn't match the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Proof that each 'duplicate' timestamp has unique hour column\n",
    "tss = df.groupby('timestamp').count().where('count > 1').sort('count', ascending=False).select('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tss.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----+---+----+------+----------+---------+---------+------------------+-------------------+-----------+------------------+------------------+---+\n",
      "|          timestamp|Year|Month|Day|Hour|Minute|Cloud Type|Dew Point|Fill Flag|        Wind Speed|     Surface Albedo|Temperature|Solar Zenith Angle|    Wind Direction|GHI|\n",
      "+-------------------+----+-----+---+----+------+----------+---------+---------+------------------+-------------------+-----------+------------------+------------------+---+\n",
      "|2009-03-08 07:30:00|2009|    3|  8|   2|    30|         7|       16|        0|3.4000000000000004|              0.149|         18|            120.06|             187.7|  0|\n",
      "|2009-03-08 07:30:00|2009|    3|  8|   3|    30|         7|       16|        0|               3.9|              0.149|         18|            131.34|             189.3|  0|\n",
      "|2008-03-09 07:30:00|2008|    3| 10|   2|    30|         7|        4|        0|               1.5|0.12300000000000001|          6|            119.62|             175.0|  0|\n",
      "|2008-03-09 07:30:00|2008|    3| 10|   3|    30|         7|        4|        0|               1.6|0.12300000000000001|          5|            130.83|             181.0|  0|\n",
      "|2012-03-11 07:30:00|2012|    3| 12|   2|    30|         6|       15|        0|3.3000000000000003|              0.147|         15|            119.23|155.20000000000005|  0|\n",
      "|2012-03-11 07:30:00|2012|    3| 12|   3|    30|         6|       15|        0|               3.0|              0.147|         15|            130.36|             164.4|  0|\n",
      "|2013-03-10 07:00:00|2013|    3| 10|   2|     0|         4|       11|        0|2.8000000000000003|              0.151|         13|            113.76|             165.3|  0|\n",
      "|2013-03-10 07:00:00|2013|    3| 10|   3|     0|         4|       11|        0|3.4000000000000004|              0.151|         13|             125.4|             165.5|  0|\n",
      "|2006-04-02 07:30:00|2006|    4|  2|   2|    30|         6|       19|        0|               1.4|0.18899999999999997|         20|            115.09|             176.0|  0|\n",
      "|2006-04-02 07:30:00|2006|    4|  2|   3|    30|         9|       19|        0|               1.6|0.18899999999999997|         20|            125.39|             181.5|  0|\n",
      "|2016-03-13 07:00:00|2016|    3| 14|   2|     0|         8|       16|        0|               3.7|0.12300000000000001|         17|            112.97|             118.6|  0|\n",
      "|2016-03-13 07:00:00|2016|    3| 14|   3|     0|         6|       16|        0|4.1000000000000005|0.12300000000000001|         17|             124.5|             127.7|  0|\n",
      "|1998-04-05 07:30:00|1998|    4|  5|   2|    30|         4|        4|        0|               1.3|              0.183|          5|            114.47|              63.8|  0|\n",
      "|1998-04-05 07:30:00|1998|    4|  5|   3|    30|         4|        4|        0|1.2000000000000002|              0.183|          5|            124.66|              67.3|  0|\n",
      "|2002-04-07 07:30:00|2002|    4|  7|   2|    30|         4|        7|        0|1.7000000000000002|              0.184|          9|            114.05|             103.7|  0|\n",
      "|2002-04-07 07:30:00|2002|    4|  7|   3|    30|         7|        7|        0|1.7000000000000002|              0.184|          8|            124.15|             106.9|  0|\n",
      "|2016-03-13 07:30:00|2016|    3| 14|   2|    30|         8|       16|        0|               3.9|0.12300000000000001|         17|            118.83|             118.6|  0|\n",
      "|2016-03-13 07:30:00|2016|    3| 14|   3|    30|         6|       16|        0|               4.3|0.12300000000000001|         17|            129.88|             127.7|  0|\n",
      "|2004-04-04 07:30:00|2004|    4|  5|   2|    30|         0|        4|        0|               1.3|              0.158|          7|            114.36|              84.9|  0|\n",
      "|2004-04-04 07:30:00|2004|    4|  5|   3|    30|         0|        4|        0|               1.4|              0.158|          6|            124.52|              92.7|  0|\n",
      "+-------------------+----+-----+---+----+------+----------+---------+---------+------------------+-------------------+-----------+------------------+------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dup_rows = df.join(tss,on='timestamp', how='right')\n",
    "dup_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_rows.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----+\n",
      "|          timestamp|Hour|count|\n",
      "+-------------------+----+-----+\n",
      "|2007-03-11 07:30:00|   3|    1|\n",
      "|2011-03-13 07:00:00|   2|    1|\n",
      "|2012-03-11 07:00:00|   2|    1|\n",
      "|2004-04-04 07:00:00|   2|    1|\n",
      "|2002-04-07 07:00:00|   3|    1|\n",
      "|1998-04-05 07:30:00|   2|    1|\n",
      "|1998-04-05 07:00:00|   3|    1|\n",
      "|2017-03-12 07:00:00|   2|    1|\n",
      "|2008-03-09 07:30:00|   3|    1|\n",
      "|2005-04-03 07:30:00|   3|    1|\n",
      "|2017-03-12 07:30:00|   2|    1|\n",
      "|2016-03-13 07:00:00|   3|    1|\n",
      "|2000-04-02 07:00:00|   3|    1|\n",
      "|2014-03-09 07:00:00|   2|    1|\n",
      "|2008-03-09 07:30:00|   2|    1|\n",
      "|1999-04-04 07:30:00|   3|    1|\n",
      "|2008-03-09 07:00:00|   3|    1|\n",
      "|2002-04-07 07:30:00|   2|    1|\n",
      "|2016-03-13 07:30:00|   3|    1|\n",
      "|2004-04-04 07:30:00|   3|    1|\n",
      "+-------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dup_rows.groupby('timestamp', 'Hour').count().sort(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These hour issues are from daylight savings time!!\n",
    "#This means that switching to UTC should get rid of the Null entries when trying to convert timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I'll update the timestamp with the uniquely identifying Year|Month|Day|Hour|Minute columns\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = (df.withColumn('Month', F.when(F.length(F.col('Month')) == 1, F.concat(F.lit('0'), F.col('Month'))).otherwise(F.col('Month')))\n",
    "        .withColumn('Day', F.when(F.length(F.col('Day')) == 1, F.concat(F.lit('0'), F.col('Day'))).otherwise(F.col('Day')))\n",
    "        .withColumn('Hour', F.when(F.length(F.col('Hour')) == 1, F.concat(F.lit('0'), F.col('Hour'))).otherwise(F.col('Hour')))\n",
    "        .withColumn('Minute', F.when(F.length(F.col('Minute')) == 1, F.concat(F.lit('0'), F.col('Minute'))).otherwise(F.col('Minute')))\n",
    "        .withColumn('time', F.to_timestamp(F.concat(*['Year', 'Month', 'Day', 'Hour', 'Minute']), format='yyyyMMddHHmm'))\n",
    "     )\n",
    "\n",
    "df = df.drop('timestamp')\n",
    "df = df.withColumnRenamed('time', 'timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+------+----------+---------+---------+----------+-------------------+-----------+------------------+--------------+---+-------------------+\n",
      "|Year|Month|Day|Hour|Minute|Cloud Type|Dew Point|Fill Flag|Wind Speed|     Surface Albedo|Temperature|Solar Zenith Angle|Wind Direction|GHI|          timestamp|\n",
      "+----+-----+---+----+------+----------+---------+---------+----------+-------------------+-----------+------------------+--------------+---+-------------------+\n",
      "|2008|   03| 09|  07|    30|         7|       -2|        0|       1.1|0.12300000000000001|         -2|             144.5|         194.5|  0|2008-03-09 07:30:00|\n",
      "+----+-----+---+----+------+----------+---------+---------+----------+-------------------+-----------+------------------+--------------+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where('timestamp == \"2008-03-09 07:30:00\"').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Hour: string (nullable = true)\n",
      " |-- Minute: string (nullable = true)\n",
      " |-- Cloud Type: integer (nullable = true)\n",
      " |-- Dew Point: integer (nullable = true)\n",
      " |-- Fill Flag: integer (nullable = true)\n",
      " |-- Wind Speed: double (nullable = true)\n",
      " |-- Surface Albedo: double (nullable = true)\n",
      " |-- Temperature: integer (nullable = true)\n",
      " |-- Solar Zenith Angle: double (nullable = true)\n",
      " |-- Wind Direction: double (nullable = true)\n",
      " |-- GHI: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|timestamp|count|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#No more duplicate timestamps.\n",
    "df.groupby('timestamp').count().where('count > 1').sort('count', ascending=False).sort('timestamp').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year                  0\n",
       "Month                 0\n",
       "Day                   0\n",
       "Hour                  0\n",
       "Minute                0\n",
       "Cloud Type            0\n",
       "Dew Point             0\n",
       "Fill Flag             0\n",
       "Wind Speed            0\n",
       "Surface Albedo        0\n",
       "Temperature           0\n",
       "Solar Zenith Angle    0\n",
       "Wind Direction        0\n",
       "GHI                   0\n",
       "timestamp             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Checking for null values\n",
    "df.toPandas().isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Now I'll make the above into cleaning functions to run on all my data.\n",
    "def gather_df(name):\n",
    "    \"\"\"Concats list of csvs from 1998 to 2018 for a given name\"\"\"\n",
    "    years = list(range(1998, 2018))\n",
    "    for ix, year in enumerate(years):\n",
    "        if ix == 0:\n",
    "            df = spark.read.csv(f\"hdfs://localhost:9000/solar_data/raw/{name}_{year}.csv\", inferSchema=True, header=True)\n",
    "        else:\n",
    "            to_append = spark.read.csv(f\"hdfs://localhost:9000/solar_data/raw/{name}_{year}.csv\", inferSchema=True, header=True)\n",
    "            df = df.union(to_append)\n",
    "    df = df.withColumnRenamed('_c0', 'timestamp')\n",
    "    return df\n",
    "\n",
    "def fix_timestamps(df):\n",
    "    \"\"\"Fixes the timestamp on daylight savings time\"\"\"\n",
    "    df = (df.withColumn('Month', F.when(F.length(F.col('Month')) == 1, F.concat(F.lit('0'), F.col('Month'))).otherwise(F.col('Month')))\n",
    "        .withColumn('Day', F.when(F.length(F.col('Day')) == 1, F.concat(F.lit('0'), F.col('Day'))).otherwise(F.col('Day')))\n",
    "        .withColumn('Hour', F.when(F.length(F.col('Hour')) == 1, F.concat(F.lit('0'), F.col('Hour'))).otherwise(F.col('Hour')))\n",
    "        .withColumn('Minute', F.when(F.length(F.col('Minute')) == 1, F.concat(F.lit('0'), F.col('Minute'))).otherwise(F.col('Minute')))\n",
    "        .withColumn('time', F.to_timestamp(F.concat(*['Year', 'Month', 'Day', 'Hour', 'Minute']), format='yyyyMMddHHmm'))\n",
    "     )\n",
    "\n",
    "    df = df.drop('timestamp')\n",
    "    df = df.withColumnRenamed('time', 'timestamp')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dyllanjr/Solar_Irradiance_Prediction/data/interim\n"
     ]
    }
   ],
   "source": [
    "%cd /home/dyllanjr/Solar_Irradiance_Prediction/data/interim/\n",
    "names = ['Arkansas', 'Arizona', 'Georgia']\n",
    "for name in names:\n",
    "    df = gather_df(name)\n",
    "    df = fix_timestamps(df)\n",
    "    df.write.csv(f\"/home/dyllanjr/Solar_Irradiance_Prediction/data/interim/{name}_interim.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('hadoop fs -copyFromLocal /home/dyllanjr/Solar_Irradiance_Prediction/data/interim /solar_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
